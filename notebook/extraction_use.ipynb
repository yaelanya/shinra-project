{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import shinra_util as util\n",
    "import word_entropy\n",
    "import feature\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['font.family'] = 'IPAGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)['entry']\n",
    "\n",
    "train_dict = util.train2dict(raw_train, \"用途\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wiki_df = pd.read_csv(\"../data/wikitext_split_sentence_with_subtitle.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>heading</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2662912</td>\n",
       "      <td>ハロン (halon) は、炭化水素の水素原子（一部または全て）がハロゲン原子で置換されたハ...</td>\n",
       "      <td>NO_SUBTITLE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2662912</td>\n",
       "      <td>ハロゲン化炭化水素 (halogenated hydrocarbon) が語源で、アメリカ陸...</td>\n",
       "      <td>NO_SUBTITLE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2662912</td>\n",
       "      <td>ハロン類 (halons)、ハロン化合物 (halon compounds) ともいう。</td>\n",
       "      <td>NO_SUBTITLE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2662912</td>\n",
       "      <td>ハロンに対し、臭素を含まず、ハロゲンがフッ素と塩素のみの化合物を、フロン（クロロフルオロカー...</td>\n",
       "      <td>NO_SUBTITLE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2662912</td>\n",
       "      <td>ただし、フロンが日本特有の語であるのに対し、ハロンは国際的に通用する名である。</td>\n",
       "      <td>NO_SUBTITLE</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _id                                           sentence      heading  \\\n",
       "0  2662912  ハロン (halon) は、炭化水素の水素原子（一部または全て）がハロゲン原子で置換されたハ...  NO_SUBTITLE   \n",
       "1  2662912  ハロゲン化炭化水素 (halogenated hydrocarbon) が語源で、アメリカ陸...  NO_SUBTITLE   \n",
       "2  2662912       ハロン類 (halons)、ハロン化合物 (halon compounds) ともいう。  NO_SUBTITLE   \n",
       "3  2662912  ハロンに対し、臭素を含まず、ハロゲンがフッ素と塩素のみの化合物を、フロン（クロロフルオロカー...  NO_SUBTITLE   \n",
       "4  2662912            ただし、フロンが日本特有の語であるのに対し、ハロンは国際的に通用する名である。  NO_SUBTITLE   \n",
       "\n",
       "   label  \n",
       "0  False  \n",
       "1  False  \n",
       "2  False  \n",
       "3  False  \n",
       "4  False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = all_wiki_df.loc[all_wiki_df._id.isin(train_dict.keys())].reset_index(drop=True)\n",
    "train_df._id = train_df._id.astype(str)\n",
    "train_df = util.labeling(train_df, train_dict)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>heading</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>6701</td>\n",
       "      <td>6701</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1524</td>\n",
       "      <td>1524</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        _id  sentence  heading\n",
       "label                         \n",
       "False  6701      6701     6701\n",
       "True   1524      1524     1524"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length : 8225\n",
      "label 1 : 1524 \tlabel 0 : 6701\n",
      "train length (filtering) : 4107\n",
      "label 1 : 1051 \tlabel 0 : 3056\n"
     ]
    }
   ],
   "source": [
    "# ブートストラップ法で得た手がかり語で学習データをフィルタリング\n",
    "\n",
    "with open(\"../dump/clue_words.pickle\", 'br') as f:\n",
    "    clue_word_by_BS = pickle.load(f)\n",
    "\n",
    "print(\"train length :\", len(train_df))\n",
    "print(\"label 1 :\", len(train_df.loc[train_df.label == 1]), \"\\tlabel 0 :\", len(train_df.loc[train_df.label == 0]))\n",
    "\n",
    "train_df = train_df.loc[train_df.sentence.str.contains(\"|\".join(clue_word_by_BS))].reset_index(drop=True)\n",
    "print(\"train length (filtering) :\", len(train_df))\n",
    "print(\"label 1 :\", len(train_df.loc[train_df.label == 1]), \"\\tlabel 0 :\", len(train_df.loc[train_df.label == 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropyを用いた手がかり語抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_clue_word = lambda hinshi: (hinshi[0] == \"名詞\" and hinshi[1] == \"サ変接続\") or (hinshi[0] == \"動詞\" and hinshi[1] == \"自立\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clue_word_df = \\\n",
    "train_df.assign(\n",
    "    clue_word = \n",
    "    train_df.apply(\n",
    "        lambda x: util.get_word_list(x.sentence, is_clue_word)\n",
    "        , axis=1\n",
    "    )\n",
    ")[[\"clue_word\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clue_word_entropy = word_entropy.word_entropy(clue_word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['代替',\n",
       " '利用',\n",
       " '添加',\n",
       " '染色',\n",
       " '保護',\n",
       " '増強',\n",
       " '応用',\n",
       " '鎮静',\n",
       " '硬化',\n",
       " '吸着',\n",
       " '組み合わせ',\n",
       " '混ぜ',\n",
       " '便秘',\n",
       " '検出',\n",
       " '改善',\n",
       " '使用',\n",
       " '解熱',\n",
       " '呈する',\n",
       " '通過',\n",
       " '助ける',\n",
       " '内服',\n",
       " '補助',\n",
       " '調整',\n",
       " '軽減',\n",
       " '予防',\n",
       " '嘔吐',\n",
       " '固定',\n",
       " '成人',\n",
       " '測定',\n",
       " '出血',\n",
       " '保存',\n",
       " '認め',\n",
       " '感染',\n",
       " '殺菌',\n",
       " '呼吸',\n",
       " '使わ',\n",
       " '治療',\n",
       " '湿',\n",
       " '用い',\n",
       " '抑える',\n",
       " '洗浄',\n",
       " '発揮',\n",
       " '承認']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 1.3\n",
    "entropy_clue_words = \\\n",
    "clue_word_entropy[\n",
    "    (clue_word_entropy.entropy_positive > alpha * clue_word_entropy.entropy_negative) & (clue_word_entropy.entropy_negative > 0)\n",
    "].clue_word.tolist()\n",
    "entropy_clue_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropyで得た手がかり語が文中に含まれているかどうか\n",
    "train_X = feature.contains_clue_word(train_df, entropy_clue_words + ['用途', '効果', '目的'])\n",
    "\n",
    "# サブタイトル中にEntropyで得た手がかり語が含まれているかどうか\n",
    "train_X[\"subtitle_cat\"] = feature.subtitle_cat(train_df, entropy_clue_words + ['用途', '効果', '目的'])\n",
    "\n",
    "# 文中にカテゴリ名・記事タイトル名と一致する名詞が含まれているどうか\n",
    "noun_list = pd.read_csv(\"../data/noun_list_in_category_and_title.csv\").noun.tolist()\n",
    "train_X[\"is_noun_cat\"] = train_df.sentence.str.contains(util.contains_patt(noun_list)).tolist()\n",
    "#train_X[\"n_noun\"] = train_df.sentence.str.findall(util.contains_patt(noun_list)).apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.6496185734913474\n",
      "precision:  0.7224633100732283\n",
      "recall:  0.5927781539155947\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "\n",
    "scores = cross_validate(model, train_X, train_y, scoring=['f1', 'precision', 'recall'], cv=5)\n",
    "\n",
    "print(\"f1: \", scores['test_f1'].mean())\n",
    "print(\"precision: \", scores['test_precision'].mean())\n",
    "print(\"recall: \", scores['test_recall'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用途の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(df):\n",
    "    # 元素名リスト(2)\n",
    "    element_list = pd.read_csv(\"../data/element_list.csv\").name.tolist()\n",
    "\n",
    "    # カテゴリページをクローリングして得た名詞リストを取得(3)\n",
    "    noun_list = pd.read_csv(\"../data/noun_list_in_category_and_title.csv\").noun.values\n",
    "    noun_list = \\\n",
    "    list(\n",
    "        set(util.flatten([[noun, re.sub(r'[\\(（].+[\\)）]', '', noun)] for noun in noun_list]))\n",
    "    )\n",
    "\n",
    "    # タイトル化合物の名称のリストを作成(4)\n",
    "    compound_list = pd.read_csv(\"../data/compound_list.csv\")['compound'].tolist()\n",
    "    compound_list = list(set(util.flatten([[compound, re.sub(r'[\\(（].+[\\)）]', '', compound)] for compound in compound_list])))\n",
    "\n",
    "    # 学習データ（特性，種類）のリストを作成(5)\n",
    "    train_character_list = util.flatten([entry['Attributes']['特性'] for entry in raw_train])\n",
    "    train_type_list = util.flatten([entry['Attributes']['種類'] for entry in raw_train])\n",
    "\n",
    "    # (3)の名詞を含んでいる名詞だけを抽出\n",
    "    use_df = df[df.use.str.contains('|'.join(noun_list))]\n",
    "\n",
    "    # (2),(4),(5)と完全一致する名詞は除外\n",
    "    use_df = \\\n",
    "    use_df[use_df.apply(\n",
    "        lambda x: True if x.use not in (train_character_list + train_type_list + compound_list + element_list) else False\n",
    "        , axis=1\n",
    "    )]\n",
    "\n",
    "    # 末尾が化合物名で終わる名詞は除外\n",
    "    patt = '.*(\\w{1,2}化)?物?((' \\\n",
    "            + '|'.join(compound_list + element_list).replace('(', '\\(').replace(')', '\\)') \\\n",
    "            + ')化?)+(化物|化合物|イオン|塩|酸)*$'\n",
    "    use_df = use_df[~use_df.use.str.match(patt)]\n",
    "\n",
    "    # 末尾が「化合物」で終わる名詞は除外\n",
    "    # 「〜の化合物」といったものが除去できる\n",
    "    patt = '.*化合物$'\n",
    "    use_df = use_df[~use_df.use.str.match(patt)]\n",
    "\n",
    "    return use_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_use_list(_id: str, sentence: str):\n",
    "    use_list = list(set(util.get_noun_list(sentence, condition=3)))\n",
    "    return use_list\n",
    "\n",
    "def get_use_df(predicted: pd.DataFrame):\n",
    "    use_df = pd.DataFrame()\n",
    "    for _, row in predicted.iterrows():\n",
    "        use_df = use_df.append(\n",
    "                    pd.DataFrame({\n",
    "                        \"_id\": row._id\n",
    "                        , \"use\": get_use_list(row._id, row.sentence)\n",
    "                    })\n",
    "                )\n",
    "\n",
    "    # 用途っぽい名詞だけ抽出\n",
    "    use_df = remove(use_df)\n",
    "    \n",
    "    return use_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.3898330531005896\n",
      "precision:  0.5671941190056339\n",
      "recall:  0.2986401907658104\n"
     ]
    }
   ],
   "source": [
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "for train_index, test_index in StratifiedKFold(n_splits=5).split(train_X, train_y):\n",
    "    X_train, X_test = train_X.loc[train_index], train_X.loc[test_index]\n",
    "    y_train, y_test = train_y[train_index], train_y[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    \n",
    "    predict_true_df = train_df.loc[test_index][predict]\n",
    "    use_df = get_use_df(predict_true_df)\n",
    "    result = util.df2dict(use_df, 'use')\n",
    "    \n",
    "    test_ids = train_df.loc[test_index, '_id'].unique()\n",
    "    score = util.validation(result, util.extract_from_dict(train_dict, test_ids))\n",
    "    \n",
    "    precision.append(score['precision'])\n",
    "    recall.append(score['recall'])\n",
    "    f1.append(score['f1'])\n",
    "    \n",
    "print(\"f1: \", np.mean(f1))\n",
    "print(\"precision: \", np.mean(precision))\n",
    "print(\"recall: \", np.mean(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.5828144458281445, 'recall': 0.3959587274290628, 'f1': 0.47155045234723336}\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_y)\n",
    "predict = model.predict(train_X)\n",
    "predict_true_df = train_df[predict]\n",
    "use_df = get_use_df(predict_true_df)\n",
    "result = util.df2dict(use_df, 'use')\n",
    "\n",
    "test_ids = train_df.loc[test_index, '_id'].unique()\n",
    "score = util.validation(result, train_dict)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier()\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wiki_filtering_df = all_wiki_df.loc[all_wiki_df.sentence.str.contains(\"|\".join(clue_word_by_BS))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropyで得た手がかり語が文中に含まれているかどうか\n",
    "X = feature.contains_clue_word(all_wiki_filtering_df, entropy_clue_words + ['用途', '効果', '目的'])\n",
    "\n",
    "# サブタイトル中にEntropyで得た手がかり語が含まれているかどうか\n",
    "X[\"subtitle_cat\"] = feature.subtitle_cat(all_wiki_filtering_df, entropy_clue_words + ['用途', '効果', '目的'])\n",
    "\n",
    "# 文中にカテゴリ名・記事タイトル名と一致する名詞が含まれているどうか\n",
    "noun_list = pd.read_csv(\"../data/noun_list_in_category_and_title.csv\").noun.tolist()\n",
    "X[\"is_noun_cat\"] = all_wiki_filtering_df.sentence.str.contains(util.contains_patt(noun_list)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X)\n",
    "predict_true_df = all_wiki_filtering_df[predict]\n",
    "use_df = get_use_df(predict_true_df)\n",
    "result = util.df2dict(use_df, 'use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/use.json\", 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

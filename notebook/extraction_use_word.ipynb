{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = pd.read_csv(\"../data/wikitext_split_sentence.csv\").groupby('_id')[['sentence']].sum().reset_index()\n",
    "wiki_data._id = wiki_data._id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\") as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist if len(item) is not 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"positive_sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(hinshi, noun):\n",
    "    if not (hinshi[0] in ['名詞', '接頭詞']) and (len(noun) == 0):\n",
    "        return False\n",
    "    elif (hinshi[0] == '名詞') and (hinshi[1] == '固有名詞') and (hinshi[2] != '一般'):\n",
    "        return False\n",
    "    elif (hinshi[0] == '名詞') and (hinshi[1] in ['代名詞', '非自立', '特殊']):\n",
    "        return False\n",
    "    elif (hinshi[0] in ['名詞', '接頭詞']) or ((hinshi[0] == '助詞') and (hinshi[1] in ['連体化', '並立助詞', '副助詞'])):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def remove_tail_adv(noun, hinshi):\n",
    "    while hinshi.pop() != '名詞':\n",
    "        noun.pop()\n",
    "        if len(hinshi) == 0:\n",
    "            break\n",
    "    \n",
    "def get_noun_list(s):\n",
    "    mecab_param = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    mecab_param.parse(\"\")\n",
    "    node = mecab_param.parseToNode(s)\n",
    "    \n",
    "    noun_list = []\n",
    "    hinshi_list = []\n",
    "    noun = []\n",
    "    while node:\n",
    "        if len(node.surface) == 0:\n",
    "            node = node.next\n",
    "            continue\n",
    "\n",
    "        hinshi = node.feature.split(',')\n",
    "        if is_noun(hinshi, noun):\n",
    "            hinshi_list.append(hinshi[0])\n",
    "            noun.append(node.surface)\n",
    "        elif len(noun) > 0:\n",
    "            remove_tail_adv(noun, hinshi_list)\n",
    "            if len(noun) == 0:\n",
    "                node = node.next\n",
    "                continue\n",
    "            \n",
    "            noun_list.append(''.join(noun))\n",
    "            hinshi_list = []\n",
    "            noun = []\n",
    "        \n",
    "        node = node.next\n",
    "    \n",
    "    if len(noun) > 0:\n",
    "        remove_tail_adv(noun, hinshi_list)\n",
    "        noun_list.append(''.join(noun))\n",
    "    \n",
    "    return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_TFIDF(entry_id, use_noun):\n",
    "    mecab_param = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    mecab_param.parse(\"\")\n",
    "    node = mecab_param.parseToNode(use_noun)\n",
    "    \n",
    "    noun = []\n",
    "    while node:\n",
    "        if len(node.surface) == 0:\n",
    "            node = node.next\n",
    "            continue\n",
    "\n",
    "        hinshi = node.feature.split(',')\n",
    "        if hinshi[0] == \"名詞\":\n",
    "            noun.append(node.surface)\n",
    "        \n",
    "        node = node.next\n",
    "    \n",
    "    \n",
    "    return TFIDF(entry_id, noun)\n",
    "\n",
    "def TFIDF(entry_id, noun):\n",
    "    lower_noun = [n.lower() for n in noun]\n",
    "    noun_list = list(set(tfidf_df.columns) & set(lower_noun))\n",
    "    if len(noun_list) == 0:\n",
    "        return 0\n",
    "        \n",
    "    return tfidf_df.loc[tfidf_df._id == str(entry_id), lower_noun].values.flatten().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_wakati_param = MeCab.Tagger(\"-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "wakati_entries = wiki_data.apply(lambda x: mecab_wakati_param.parse(x.sentence).strip(), axis=1).tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern='[^\\s]+')\n",
    "tfidf_vec = vectorizer.fit_transform(wakati_entries).toarray()\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_vec, columns=features).assign(_id = wiki_data._id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/pandas/core/indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for _, (_id, s) in test_df.iterrows():\n",
    "    use_list = get_noun_list(s)\n",
    "    TFIDF_list = [noun_TFIDF(_id, use_noun) for use_noun in use_list]\n",
    "    tmp_df = pd.DataFrame({\"_id\": _id, \"use\": use_list, \"TFIDF\": TFIDF_list})\n",
    "    df = df.append(tmp_df)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NITEに存在したカテゴリ名のうち，学習データの用途に出現していない名詞を除去したリストを作成(1)\n",
    "NITE_df = pd.read_csv(\"../data/NITE_existence_list.csv\")\n",
    "unuse_df = \\\n",
    "pd.merge(\n",
    "    NITE_df.loc[NITE_df.label == 1].rename(columns={\"category\": \"noun\"})\n",
    "    , pd.DataFrame({\"noun\": noun_list})\n",
    "    , on='noun'\n",
    ")\n",
    "unuse_list = list(set(unuse_df.noun.tolist()) - set(flatten(train_use_dict.values())))\n",
    "\n",
    "# 元素名リスト(2)\n",
    "element_list = pd.read_csv(\"../data/element_list.csv\").name.tolist()\n",
    "\n",
    "# カテゴリページをクローリングして得た名詞リストを取得(3)\n",
    "noun_list = pd.read_csv(\"../data/noun_list_in_category_and_title.csv\").noun.values\n",
    "noun_list = \\\n",
    "list(\n",
    "    set(flatten([[noun, re.sub(r'[\\(（].+[\\)）]', '', noun)] for noun in noun_list]))\n",
    ")\n",
    "\n",
    "# タイトル化合物の名称のリストを作成(4)\n",
    "compound_list = pd.read_csv(\"../data/compound_list.csv\")['compound'].tolist()\n",
    "compound_list = list(set(flatten([[compound, re.sub(r'[\\(（].+[\\)）]', '', compound)] for compound in compound_list])))\n",
    "\n",
    "# 学習データ（特性，種類）のリストを作成(5)\n",
    "train_character_list = flatten([entry['Attributes']['特性'] for entry in train['entry']])\n",
    "train_type_list = flatten([entry['Attributes']['種類'] for entry in train['entry']])\n",
    "\n",
    "# (3)の名詞を含んでいる名詞だけを抽出\n",
    "use_df = df[df.use.str.contains('|'.join(noun_list))]\n",
    "\n",
    "# (1),(2),(4),(5)と完全一致する名詞は除外\n",
    "use_df = \\\n",
    "use_df[use_df.apply(\n",
    "    lambda x: True if x.use not in (train_character_list + train_type_list + compound_list + unuse_list + element_list) else False\n",
    "    , axis=1\n",
    ")]\n",
    "\n",
    "# 末尾が化合物名で終わる名詞は除外\n",
    "patt = '.*(\\w{1,2}化)?物?((' \\\n",
    "        + '|'.join(compound_list + element_list).replace('(', '\\(').replace(')', '\\)') \\\n",
    "        + ')化?)+(化物|化合物|イオン|塩|酸)*$'\n",
    "use_df = use_df[~use_df.use.str.match(patt)]\n",
    "\n",
    "# 末尾が「化合物」で終わる名詞は除外\n",
    "# 「〜の化合物」といったものが除去できる\n",
    "patt = '.*化合物$'\n",
    "use_df = use_df[~use_df.use.str.match(patt)]\n",
    "\n",
    "use_df['_id'] = use_df._id.astype(str)\n",
    "\n",
    "train_use_dict = \\\n",
    "dict(zip(\n",
    "    [str(entry['WikipediaID']) for entry in train['entry']]\n",
    "    , [entry['Attributes']['用途'] for entry in train['entry']]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1425\n",
      "0.7024561403508772\n"
     ]
    }
   ],
   "source": [
    "is_contain_list1 = []\n",
    "is_contain_list2 = []\n",
    "predict_df = pd.DataFrame()\n",
    "for _id, group in use_df.groupby('_id'):\n",
    "    if not train_use_dict.get(_id):\n",
    "        continue\n",
    "    \n",
    "    group = group[~group.duplicated('use')]\n",
    "    true_use_list = train_use_dict[_id]\n",
    "    \n",
    "    # contains extraction data in train data\n",
    "    use_str = ','.join(true_use_list)\n",
    "    result1 = group.apply(lambda x: True if re.search(x.use, use_str) else False, axis=1).tolist()\n",
    "    is_contain_list1 += result1   \n",
    "\n",
    "    # contains train data in extraction data\n",
    "    r2 = '|'.join(true_use_list).replace('(', '\\(').replace(')', '\\)')\n",
    "    result2 = group.use.str.contains(r2).tolist()\n",
    "    is_contain_list2 += result2\n",
    "    \n",
    "    predict_df = predict_df.append(group.assign(label = np.array(result1) + np.array(result2)))\n",
    "\n",
    "print(len(predict_df))\n",
    "print(predict_df.label.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.to_csv('../data/extracted_use_list_in_train_data.csv', index=False)\n",
    "predict_df[predict_df.label == False].to_csv('../data/extracted_use_list_in_train_data_only_false.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

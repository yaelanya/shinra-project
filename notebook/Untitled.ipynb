{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\") as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist if len(item) is not 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_param = MeCab.Tagger(\"-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"positive_sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(hinshi, noun):\n",
    "    if not (hinshi[0] in ['名詞', '接頭詞']) and (len(noun) == 0):\n",
    "        return False\n",
    "    elif (hinshi[0] == '名詞') and (hinshi[1] == '固有名詞') and (hinshi[2] != '一般'):\n",
    "        return False\n",
    "    elif (hinshi[0] == '名詞') and (hinshi[1] in ['代名詞', '非自立', '特殊']):\n",
    "        return False\n",
    "    elif (hinshi[0] in ['名詞', '接頭詞']) or ((hinshi[0] == '助詞') and (hinshi[1] in ['連体化', '並立助詞', '副助詞'])):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def remove_tail_adv(noun, hinshi):\n",
    "    while hinshi.pop() != '名詞':\n",
    "        noun.pop()\n",
    "        if len(hinshi) == 0:\n",
    "            break\n",
    "    \n",
    "def get_noun_list(s):\n",
    "    node = mecab_param.parseToNode(s)\n",
    "    noun_list = []\n",
    "    hinshi_list = []\n",
    "    noun = []\n",
    "    while node:\n",
    "        if len(node.surface) == 0:\n",
    "            node = node.next\n",
    "            continue\n",
    "\n",
    "        hinshi = node.feature.split(',')\n",
    "        if is_noun(hinshi, noun):\n",
    "            hinshi_list.append(hinshi[0])\n",
    "            noun.append(node.surface)\n",
    "        elif len(noun) > 0:\n",
    "            remove_tail_adv(noun, hinshi_list)\n",
    "            noun_list.append(''.join(noun))\n",
    "            hinshi_list = []\n",
    "            noun = []\n",
    "        \n",
    "        node = node.next\n",
    "    \n",
    "    if len(noun) > 0:\n",
    "        remove_tail_adv(noun, hinshi_list)\n",
    "        noun_list.append(''.join(noun))\n",
    "    \n",
    "    return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for _, (_id, s) in test_df.iterrows():\n",
    "    df = df.append(pd.DataFrame({\"_id\": _id, \"use\": get_noun_list(s)}))\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = pd.read_csv(\"../data/noun_list_in_category_and_title.csv\").noun.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_character_list = flatten([entry['Attributes']['特性'] for entry in train['entry']])\n",
    "train_type_list = flatten([entry['Attributes']['種類'] for entry in train['entry']])\n",
    "compound_list = pd.read_csv(\"../data/compound_list.csv\")['compound'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df = df[df.use.str.contains('|'.join(noun_list))]\n",
    "use_df = use_df[~use_df.use.str.match('|'.join(train_character_list + train_type_list + compound_list).replace('(', '\\(').replace(')', '\\)'))]\n",
    "use_df['_id'] = use_df._id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use_dict = \\\n",
    "dict(zip(\n",
    "    [str(entry['WikipediaID']) for entry in train['entry']]\n",
    "    , [entry['Attributes']['用途'] for entry in train['entry']]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_contain_list1 = []\n",
    "is_contain_list2 = []\n",
    "predict_df = pd.DataFrame()\n",
    "for _id, group in use_df.groupby('_id'):\n",
    "    if not train_use_dict.get(_id):\n",
    "        continue\n",
    "    \n",
    "    group = group[~group.duplicated('use')]\n",
    "    true_use_list = train_use_dict[_id]\n",
    "    \n",
    "    # contains extraction data in train data\n",
    "    #r1 = '|'.join(group.use.tolist()).replace('(', '\\(').replace(')', '\\)')\n",
    "    use_str = ','.join(true_use_list)\n",
    "    result1 = group.apply(lambda x: True if re.search(x.use, use_str) else False, axis=1).tolist()\n",
    "    is_contain_list1 += result1   \n",
    "\n",
    "    # contains train data in extraction data\n",
    "    r2 = '|'.join(true_use_list).replace('(', '\\(').replace(')', '\\)')\n",
    "    result2 = group.use.str.contains(r2).tolist()\n",
    "    is_contain_list2 += result2\n",
    "    \n",
    "    predict_df = predict_df.append(group.assign(label = np.array(result1) + np.array(result2)))\n",
    "\n",
    "is_contain_list1 = np.array(is_contain_list1)\n",
    "is_contain_list2 = np.array(is_contain_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5549618320610687\n",
      "0.4549618320610687\n",
      "0.6916030534351145\n"
     ]
    }
   ],
   "source": [
    "print(sum(is_contain_list1) / len(is_contain_list1))\n",
    "print(sum(is_contain_list2) / len(is_contain_list2))\n",
    "print(sum(is_contain_list1 + is_contain_list2) / len(is_contain_list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df.to_csv('extracted_use_list_in_train_data.csv', index=False)\n",
    "predict_df[predict_df.label == False].to_csv('extracted_use_list_in_train_data_only_false.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
